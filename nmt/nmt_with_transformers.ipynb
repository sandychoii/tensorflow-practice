{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfDn5Ec1BaaY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, LSTM, Dense\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vfK-5mUBcMJ"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 20000\n",
        "ENGLISH_SEQUENCE_LENGTH = 64\n",
        "FRENCH_SEQUENCE_LENGTH = 64\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE=8\n",
        "HIDDEN_UNITS = 256"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oO0gC49B6Y-a"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDIIG4gA5bGg",
        "outputId": "b05929da-4974-453b-c059-845bfa76b4f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-18 13:30:17--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7420323 (7.1M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.08M  20.1MB/s    in 0.4s    \n",
            "\n",
            "2023-07-18 13:30:17 (20.1 MB/s) - ‘fra-eng.zip’ saved [7420323/7420323]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvNXEGO26JQK",
        "outputId": "9128dd68-8c44-4084-da48-42ea722fb67a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgKY7xHL6T8h",
        "outputId": "dda0d55f-94b8-474f-9210-4e0b82652d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "217975 /content/dataset/fra.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -l /content/dataset/fra.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7xmy7Vb6NCV"
      },
      "outputs": [],
      "source": [
        "!head -10000 /content/dataset/fra.txt > /content/dataset/fra_10000.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_39605BFNk"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf9JUwY-YpI3",
        "outputId": "997c1fa5-981a-48e7-e0b9-fb868ae55548"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_dataset = tf.data.TextLineDataset(\"/content/dataset/fra_10000.txt\")\n",
        "text_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snv_dM-2aYTR"
      },
      "outputs": [],
      "source": [
        "english_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aFOE8IAbSkC"
      },
      "outputs": [],
      "source": [
        "french_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy44fl-KcH4E"
      },
      "outputs": [],
      "source": [
        "def selector(input_text):\n",
        "  split_text = tf.strings.split(input_text, '\\t')\n",
        "  return {'input_1':split_text[0:1], 'input_2': '[start] ' + split_text[1:2]}, split_text[1:2]+' [end]'\n",
        "  # {english, french(with start)}, french(with end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnBYhak8cVy9"
      },
      "outputs": [],
      "source": [
        "split_dataset = text_dataset.map(selector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvtskdL_cZz-",
        "outputId": "c7921d17-a5c4-4ac6-f859-fd7be722ee05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! [end]'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] Marche.'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. [end]'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] En route !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! [end]'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "for i in split_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTlSs7fLr-km"
      },
      "outputs": [],
      "source": [
        "def separator(input_text):\n",
        "  split_text = tf.strings.split(input_text, '\\t')\n",
        "  return split_text[0:1], '[start] ' + split_text[1:2]+' [end]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T25y0AxGsL70"
      },
      "outputs": [],
      "source": [
        "init_dataset = text_dataset.map(separator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BdqtDKjsR8_",
        "outputId": "b8b71eea-22d0-478c-c61d-fcb1e2635036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] Va ! [end]'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] Marche. [end]'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] En route ! [end]'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNUWQXjVbZTj"
      },
      "outputs": [],
      "source": [
        "english_training_data = init_dataset.map(lambda x,y : x) # input x,y and output x # only for english\n",
        "english_vectorize_layer.adapt(english_training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-VvhYD2JooYp",
        "outputId": "6c69f937-c5db-4305-9ab2-1866e3452165"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'we'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the vectorize layer\n",
        "english_vectorize_layer.get_vocabulary()[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg3C1P8Ico1w"
      },
      "outputs": [],
      "source": [
        "french_training_data = init_dataset.map(lambda x,y : y) # input x,y and output x # only for english\n",
        "french_vectorize_layer.adapt(french_training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO5ev4F9cx4v"
      },
      "outputs": [],
      "source": [
        "def vectorizer(inputs, output):\n",
        "  return {'input_1' :english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2': french_vectorize_layer(inputs['input_2'])}, french_vectorize_layer(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6PIbqDdoYuC"
      },
      "outputs": [],
      "source": [
        "dataset = split_dataset.map(vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJvbG2Gss_aL",
        "outputId": "08915e74-99b0-449b-9366-967e48f70c27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[ 2, 39,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>}, <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[39,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>)\n",
            "({'input_1': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[  2, 224,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[224,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>)\n",
            "({'input_1': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[  2,  22, 270,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[ 22, 270,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>)\n"
          ]
        }
      ],
      "source": [
        "# check if each inputs are mapped with the adapted vectorizer\n",
        "for i in dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jgFS-MqDAzKb"
      },
      "source": [
        "### Batch/Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyPS6TveAy9a"
      },
      "outputs": [],
      "source": [
        "dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "NUM_BATCHES=int(10000/BATCH_SIZE)\n",
        "train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n",
        "val_dataset=dataset.skip(int(0.9*NUM_BATCHES))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rs9AcvTgwK-r"
      },
      "source": [
        "## Self-attention"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Problem of seq2seq model \n",
        "    - When transferring information from the encoder to the decoder, relying on just the single context vector will cause much information to be lost by the time we get to the last unit.\n",
        "- Attention mechanism\n",
        "    - Find how similar words are in the encoder and with the decoder.\n",
        "    - Attention Score: How each encoder tokens influences the decoder token\n",
        "    - Query: Previous hidden state of decoder\n",
        "    - Key, Value: Encoder hidden state\n",
        "- Self-attention\n",
        "    - Find how similar is each and every word in the input to one another\n",
        "    - Remove RNN\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See this:https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J7nzQF4BIgQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
