{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "v4rT1trcZCBD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prep"
      ],
      "metadata": {
        "id": "hNB1JPt5YeVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC7EnZUPYNJJ",
        "outputId": "b83739fc-52c6-4f70-bb6c-85e2ed88deef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-10 11:46:56--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7420323 (7.1M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.08M  33.9MB/s    in 0.2s    \n",
            "\n",
            "2023-07-10 11:46:56 (33.9 MB/s) - ‘fra-eng.zip’ saved [7420323/7420323]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CL8QU2WYUZR",
        "outputId": "16307e21-a77f-4087-d094-4606decc3984"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head /content/dataset/fra.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Lc32osYmrT",
        "outputId": "de7783ef-7305-499a-d6bd-8e8d56e36b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
            "Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\n",
            "Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\n",
            "Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\n",
            "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
            "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
            "Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
            "Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
            "Run!\tPrenez vos jambes à vos cous !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077449 (sacredceltic)\n",
            "Run!\tFile !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077454 (sacredceltic)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "TWNl6O8MYu9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = tf.data.TextLineDataset(\"/content/dataset/fra.txt\")\n",
        "text_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf9JUwY-YpI3",
        "outputId": "8f4b8567-5e21-4a36-9721-c3d7faa47e83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the longest sequence\n",
        "for i in text_dataset.skip(190000):\n",
        "    print(len(tf.strings.split(i, \" \")))"
      ],
      "metadata": {
        "id": "DJE5qpW2ZAvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 20000\n",
        "ENGLISH_SEQUENCE_LENGTH = 64\n",
        "FRENCH_SEQUENCE_LENGTH = 64\n",
        "EMBEDDING_DIM = 300"
      ],
      "metadata": {
        "id": "5qpY7EHnZOo9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization?hl=en\n",
        "\n",
        "Maps text features to integer sequences"
      ],
      "metadata": {
        "id": "euhUEs-9a7Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "snv_dM-2aYTR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "_aFOE8IAbSkC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://d2l.ai/_images/seq2seq.svg)"
      ],
      "metadata": {
        "id": "YYJ1f3JYrOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def selector(input_text):\n",
        "  split_text = tf.strings.split(input_text, '\\t')\n",
        "  return {'input_1':split_text[0:1], 'input_2': '[start] ' + split_text[1:2]}, split_text[1:2]+' [end]'"
      ],
      "metadata": {
        "id": "Oy44fl-KcH4E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = text_dataset.map(selector)"
      ],
      "metadata": {
        "id": "EnBYhak8cVy9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separator(input_text):\n",
        "  split_text = tf.strings.split(input_text, '\\t')\n",
        "  return split_text[0:1], '[start] ' + split_text[1:2]+' [end]'"
      ],
      "metadata": {
        "id": "xTlSs7fLr-km"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset = text_dataset.map(separator)"
      ],
      "metadata": {
        "id": "T25y0AxGsL70"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in split_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvtskdL_cZz-",
        "outputId": "63e07e46-79f8-40ce-bc11-5df6948a5039"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! [end]'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] Marche.'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. [end]'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] En route !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! [end]'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BdqtDKjsR8_",
        "outputId": "3ed47905-0171-4322-def7-f272b88f7fdd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] Va ! [end]'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] Marche. [end]'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[start] En route ! [end]'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data = init_dataset.map(lambda x,y : x) # input x,y and output x # only for english\n",
        "english_vectorize_layer.adapt(english_training_data)"
      ],
      "metadata": {
        "id": "fNUWQXjVbZTj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_training_data = split_dataset.map(lambda x,y : y) # input x,y and output x # only for english\n",
        "french_vectorize_layer.adapt(french_training_data)"
      ],
      "metadata": {
        "id": "eg3C1P8Ico1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(inputs, output):\n",
        "  return {'input_1' :english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2': french_vectorize_layer(inputs['input_1'])}, french_vectorize_layer(output)"
      ],
      "metadata": {
        "id": "dO5ev4F9cx4v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = split_dataset.map(vectorizer)"
      ],
      "metadata": {
        "id": "C6PIbqDdoYuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "OJvbG2Gss_aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorize_layer.get_vocabulary()[832]"
      ],
      "metadata": {
        "id": "-VvhYD2JooYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jw3p7Yl8otlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}